{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAQTXgCebjkc5kIRTYHI4F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMUtSGnhgG79",
        "outputId": "33567e1f-0818-4ed1-d672-e123dea7e584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUbCRv6IfzFA",
        "outputId": "743cc8b1-49e8-4874-d4d5-c2851fdabb85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run', 'ran', 'adjust', 'raft', 'abil']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = ['runs','ran','adjustable','rafting','ability']\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "words = ['runs','ran','running','adjustable','rafting','ability']\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv84b4qpgQ5l",
        "outputId": "75f16b33-06ea-4043-fba2-2c371c557890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run', 'ran', 'run', 'adjust', 'raft', 'abl']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['runs','ran','running','adjustable','rafting','ability']\n",
        "lemmatize_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "lemmatize_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOkmg9i6gaF4",
        "outputId": "bb0a8aa1-3e98-45f3-c5ca-03021138aea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run', 'ran', 'running', 'adjustable', 'rafting', 'ability']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize,pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fzczc6pggjq",
        "outputId": "c541d998-21b2-487d-8eb3-a30207985dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else :\n",
        "        return wordnet.NOUN\n",
        "\n"
      ],
      "metadata": {
        "id": "q8sxG-Ysg4gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_passage(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(tag)) for word,tag in pos_tags]\n",
        "    lemmatized_sentence= ' '.join(lemmatized_words)\n",
        "    return lemmatized_sentence\n",
        "\n",
        "text = \"There is nothing either good or bad but thinking make it so \"\n",
        "result = lemmatize_passage(text)\n",
        "print(\"Original:\",text)\n",
        "print(\"tokenized:\",word_tokenize(text))\n",
        "print(\"Lemmatized:\",result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfNOjp7Pg8rK",
        "outputId": "0b38672e-da9d-489f-b327-45fd421987e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: There is nothing either good or bad but thinking make it so \n",
            "tokenized: ['There', 'is', 'nothing', 'either', 'good', 'or', 'bad', 'but', 'thinking', 'make', 'it', 'so']\n",
            "Lemmatized: There be nothing either good or bad but think make it so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uYX1R2ag_PU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}